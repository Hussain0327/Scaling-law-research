Mechanistic Interpretability Research
Mechanistic interpretability is about understanding how neural networks implement specific computations at the level of neurons, layers, and circuits. It is central to AGI alignment and capability development and is one of the most accessible high-impact areas for undergraduates.

With this direction, I can:
• Work on open small and mid-scale models such as GPT-2, Qwen-3B, and DeepSeek-VL2 using standard tooling.
• Run all experiments on a single laptop or modest cloud instance instead of large clusters.
• Produce genuinely new results by discovering and validating previously unstudied circuits.
• Reach “workshop-publishable” results on a one-semester timeline with careful scoping and evaluation.
• Contribute directly to AGI-relevant questions like alignment, internal representations, and emergent reasoning.
Project: Discover and causally test a circuit for two digit addition in GPT-2 small using TransformerLens, and compare behaviors to a smaller open model such as Qwen-1.5B or DeepSeek-VL2 text backbone.